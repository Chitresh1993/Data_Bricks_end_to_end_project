{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe09e091-f52b-440b-8155-3afe754553c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1541\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1540\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[0;32m-> 1541\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mAnalyzePlan(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\u001B[1;32m   1542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n",
       "\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    270\u001B[0m     request: Any,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n",
       "\u001B[1;32m    278\u001B[0m         request,\n",
       "\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n",
       "\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n",
       "\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n",
       "\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n",
       "\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m    284\u001B[0m     )\n",
       "\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n",
       "\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n",
       "\u001B[1;32m    331\u001B[0m )\n",
       "\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n",
       "\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n",
       "\u001B[1;32m    316\u001B[0m         request,\n",
       "\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n",
       "\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n",
       "\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n",
       "\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n",
       "\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n",
       "\u001B[1;32m    322\u001B[0m     )\n",
       "\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m   1192\u001B[0m (\n",
       "\u001B[1;32m   1193\u001B[0m     state,\n",
       "\u001B[1;32m   1194\u001B[0m     call,\n",
       "\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n",
       "\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n",
       "\u001B[1;32m   1197\u001B[0m )\n",
       "\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n",
       "\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
       "\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAVAILABLE\n",
       "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-08-12T07:10:21.466912036+00:00\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7400359278802062>, line 35\u001B[0m\n",
       "\u001B[1;32m     32\u001B[0m geography_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msilver_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/silver_geography\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     33\u001B[0m date_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msilver_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/silver_date\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 35\u001B[0m orders_bz\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\u001B[1;32m     36\u001B[0m returns_bz\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/monkey_patches.py:67\u001B[0m, in \u001B[0;36mapply_dataframe_display_patch.<locals>.df_display\u001B[0;34m(df, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdf_display\u001B[39m(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m     64\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    df.display() is an alias for display(df). Run help(display) for more information.\u001B[39;00m\n",
       "\u001B[1;32m     66\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 67\u001B[0m     display(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001B[39;00m\n",
       "\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark_connect_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:103\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n",
       "\u001B[1;32m    100\u001B[0m         e\n",
       "\u001B[1;32m    101\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython shell encountered an error or was missing data, please restart the notebook or contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    102\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n",
       "\u001B[1;32m    105\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.11/functools.py:1001\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, instance, owner)\u001B[0m\n",
       "\u001B[1;32m    999\u001B[0m val \u001B[38;5;241m=\u001B[39m cache\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname, _NOT_FOUND)\n",
       "\u001B[1;32m   1000\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n",
       "\u001B[0;32m-> 1001\u001B[0m     val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc(instance)\n",
       "\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1003\u001B[0m         cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname] \u001B[38;5;241m=\u001B[39m val\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2017\u001B[0m, in \u001B[0;36mDataFrame.isStreaming\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2014\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mcached_property\n",
       "\u001B[1;32m   2015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21misStreaming\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
       "\u001B[1;32m   2016\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2017\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_streaming\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mquery)\u001B[38;5;241m.\u001B[39mis_streaming\n",
       "\u001B[1;32m   2018\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2019\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1546\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1545\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1546\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2055\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\u001B[1;32m   2056\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n",
       "\u001B[1;32m   2057\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n",
       "\u001B[1;32m   2058\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2060\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   2061\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1539\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1531\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   1532\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUNSUPPORTED_OPERATION\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1533\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[1;32m   1534\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moperation\u001B[39m\u001B[38;5;124m\"\u001B[39m: method,\n",
       "\u001B[1;32m   1535\u001B[0m         },\n",
       "\u001B[1;32m   1536\u001B[0m     )\n",
       "\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 1539\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n",
       "\u001B[1;32m   1540\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[1;32m   1541\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mAnalyzePlan(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n",
       "\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n",
       "\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n",
       "\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RetriesExceeded",
        "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "metadata": {
        "errorSummary": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "RETRIES_EXCEEDED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1541\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1540\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[0;32m-> 1541\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mAnalyzePlan(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n\u001B[1;32m   1542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    270\u001B[0m     request: Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n\u001B[1;32m    278\u001B[0m         request,\n\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    284\u001B[0m     )\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n\u001B[1;32m    331\u001B[0m )\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n\u001B[1;32m    316\u001B[0m         request,\n\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n\u001B[1;32m    322\u001B[0m     )\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m   1192\u001B[0m (\n\u001B[1;32m   1193\u001B[0m     state,\n\u001B[1;32m   1194\u001B[0m     call,\n\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001B[1;32m   1197\u001B[0m )\n\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
        "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-08-12T07:10:21.466912036+00:00\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-7400359278802062>, line 35\u001B[0m\n\u001B[1;32m     32\u001B[0m geography_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msilver_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/silver_geography\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     33\u001B[0m date_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msilver_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/silver_date\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 35\u001B[0m orders_bz\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n\u001B[1;32m     36\u001B[0m returns_bz\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/monkey_patches.py:67\u001B[0m, in \u001B[0;36mapply_dataframe_display_patch.<locals>.df_display\u001B[0;34m(df, *args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdf_display\u001B[39m(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     64\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    df.display() is an alias for display(df). Run help(display) for more information.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m     display(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark_connect_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:103\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    100\u001B[0m         e\n\u001B[1;32m    101\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython shell encountered an error or was missing data, please restart the notebook or contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    102\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df\u001B[38;5;241m.\u001B[39misStreaming:\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n\u001B[1;32m    105\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "File \u001B[0;32m/usr/lib/python3.11/functools.py:1001\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, instance, owner)\u001B[0m\n\u001B[1;32m    999\u001B[0m val \u001B[38;5;241m=\u001B[39m cache\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname, _NOT_FOUND)\n\u001B[1;32m   1000\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n\u001B[0;32m-> 1001\u001B[0m     val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc(instance)\n\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1003\u001B[0m         cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrname] \u001B[38;5;241m=\u001B[39m val\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2017\u001B[0m, in \u001B[0;36mDataFrame.isStreaming\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2014\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mcached_property\n\u001B[1;32m   2015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21misStreaming\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m   2016\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2017\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_streaming\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mquery)\u001B[38;5;241m.\u001B[39mis_streaming\n\u001B[1;32m   2018\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2019\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1546\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1545\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1546\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2055\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n\u001B[1;32m   2056\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n\u001B[1;32m   2057\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m   2058\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2060\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2061\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1539\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1531\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   1532\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUNSUPPORTED_OPERATION\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1533\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m   1534\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moperation\u001B[39m\u001B[38;5;124m\"\u001B[39m: method,\n\u001B[1;32m   1535\u001B[0m         },\n\u001B[1;32m   1536\u001B[0m     )\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1539\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[1;32m   1541\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mAnalyzePlan(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Create Gold area\n",
    "# spark.sql(\"CREATE SCHEMA IF NOT EXISTS globalsuperstore.silver\")\n",
    "\n",
    "# Create Volume\n",
    "# spark.sql(\"CREATE VOLUME IF NOT EXISTS globalsuperstore.silver.silver_superstore\")\n",
    "\n",
    "\n",
    "# Bronze paths (your existing)\n",
    "bronze_base = \"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore\"\n",
    "orders_bronze  = spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore/orders_bronze_cm/\")\n",
    "returns_bronze = spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore/returns_bronze_cm/\")\n",
    "\n",
    "silver_base = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore\"\n",
    "\n",
    "\n",
    "# Load \n",
    "orders_bz  = spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore/orders_bronze_cm/\")\n",
    "returns_bz = spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore/returns_bronze_cm/\")\n",
    "\n",
    "\n",
    "## Defining the source schema \n",
    "# Defining the folder name for delta files \n",
    "\n",
    "silver_base_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore\"\n",
    "orders_path  = f\"{silver_base}/silver_order\"\n",
    "returns_path = f\"{silver_base}/silver_return\"\n",
    "customer_path = f\"{silver_base}/silver_customer\"\n",
    "product_path = f\"{silver_base}/silver_product\"\n",
    "geography_path = f\"{silver_base}/silver_geography\"\n",
    "date_path = f\"{silver_base}/silver_date\"\n",
    "\n",
    "orders_bz.limit(2).display()\n",
    "returns_bz.limit(2).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3519bdc-1be4-487f-8cd4-0578681c665e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## customer dim\n",
    "# Transformation\n",
    "\n",
    "dim_customer = (orders_bz\n",
    "  .select(\n",
    "    F.col(\"Customer ID\").alias(\"Customer_ID\"),\n",
    "    F.col(\"Customer Name\").alias(\"Customer_Name\"),\n",
    "    \"Segment\"\n",
    "  )\n",
    "  .dropDuplicates()\n",
    "  .withColumn(\n",
    "    \"customer_sk\",\n",
    "    F.abs(F.xxhash64(F.concat(F.col(\"Customer_ID\"), F.col(\"Customer_Name\"), F.col(\"Segment\")))).cast(\"long\")\n",
    "  )\n",
    ")\n",
    "\n",
    "## Writing the Delta files in silver schema\n",
    "# Writing order table\n",
    "\n",
    "(dim_customer.write.format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\")\n",
    "  .option(\"delta.columnMapping.mode\", \"name\")\n",
    "  .option(\"delta.minReaderVersion\", \"2\")\n",
    "  .option(\"delta.minWriterVersion\", \"5\")\n",
    "  .option(\"maxRetries\", \"10\")  # Increase the retry limit\n",
    "  .save(customer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5891014a-71fd-4b9a-be88-e4ea1bc5ea37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Product dim\n",
    "# Transformation\n",
    "\n",
    "dim_product = (orders_bz\n",
    "                    .select(F.col(\"Product Name\").alias(\"Product_Name\"), F.col(\"Product ID\").alias(\"Product_ID\"), 'Category', 'Sub-Category')\n",
    "                    .withColumn('product_sk', F.abs(F.xxhash64(F.concat(F.col(\"Product_Name\"),  F.col(\"Product_ID\"), F.col(\"Category\"), F.col(\"Sub-Category\") ) ) ) .cast(\"long\") )\n",
    "                    .dropDuplicates()\n",
    ")\n",
    "\n",
    "## Writing the Delta files in silver schema\n",
    "# Writing product table\n",
    "\n",
    "(dim_product.write.format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\",\"true\")\n",
    "  .option(\"delta.columnMapping.mode\",\"name\")\n",
    "  .option(\"delta.minReaderVersion\",\"2\")\n",
    "  .option(\"delta.minWriterVersion\",\"5\")\n",
    "  .save(product_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d338ce0-6455-4e22-bdfe-ad1731388585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  ## Geography dim\n",
    "# Transformation\n",
    "\n",
    "dim_geography = (orders_bz.\n",
    "                        select(\n",
    "                            \"City\",\"State\",\"Country\",\"Region\",F.col(\"Postal Code\").alias(\"Zip_Code\")\n",
    "                            )\n",
    "                        \n",
    "                        .withColumn(\"geography_sk\", F.abs(F.xxhash64(\"City\",\"State\",\"Country\",\"Region\",\"Zip_Code\")).cast(\"long\"))\n",
    "                        .dropDuplicates()\n",
    "                        \n",
    "\n",
    ")\n",
    "\n",
    "# Writing the Delta files in silver schema\n",
    "# Writing product table\n",
    "\n",
    "(dim_geography.write.format(\"delta\").\n",
    " mode(\"overwrite\").\n",
    " option(\"overwriteSchema\",\"true\").\n",
    " option(\"delta.columnMapping.mode\",\"name\").\n",
    " option(\"delta.minReaderVersion\",\"2\").\n",
    " option(\"delta.minWriterVersion\",\"5\").\n",
    " save(geography_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1acf5be-7d99-4e0d-9663-1e3c2d621964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Return Dim\n",
    "# Transformation\n",
    "\n",
    "dim_returns = (returns_bz\n",
    "                        .select(F.col(\"Order ID\").alias(\"Order_ID\"),\"Market\",\"Returned\")\n",
    "                        .dropDuplicates()\n",
    "                               )\n",
    "\n",
    "\n",
    "# Writing the Delta files in silver schema\n",
    "# Writing return table\n",
    "\n",
    "\n",
    "(dim_returns.write.format(\"delta\").\n",
    " mode(\"overwrite\").\n",
    " option(\"overwriteSchema\",\"true\").\n",
    " option(\"delta.columnMapping.mode\",\"name\").\n",
    " option(\"delta.minReaderVersion\",\"2\").\n",
    " option(\"delta.minWriterVersion\",\"5\").\n",
    " save(returns_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f23be2-33d5-432f-8486-b9b72eb36986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Order Fact\n",
    "# Transformation\n",
    "\n",
    "fact_orders = (orders_bz\n",
    "               .select(\n",
    "                    F.col(\"Row ID\").alias(\"Row_ID\"),\n",
    "                    F.col(\"Order ID\").alias(\"Order_ID\"),\n",
    "                    F.col(\"Order Date\").alias(\"Order_Date\"),\n",
    "                    F.col(\"Ship Date\").alias(\"Ship_Date\"),\n",
    "                    F.col(\"Ship Mode\").alias(\"Ship_Mode\"),\n",
    "                    \"Sales\", \"Quantity\", \"Discount\", \"Profit\", \n",
    "                    F.col(\"Shipping Cost\").alias(\"Shipping_Cost\"),\n",
    "                    F.col(\"Order Priority\").alias(\"Order_Priority\"),\n",
    "                    \"City\", \"State\", \"Country\", \"Region\", \"Product Name\", \"Product ID\", \"Category\", \"Sub-Category\", \"Customer ID\", \"Customer Name\", \"Segment\",\"Postal Code\")\n",
    "                   \n",
    "               .withColumn(\"geography_sk\", F.abs(F.xxhash64(\"City\",\"State\",\"Country\",\"Region\",\"Postal Code\")).cast(\"long\"))\n",
    "               .withColumn('product_sk', F.abs(F.xxhash64(F.concat(F.col(\"Product Name\"), F.col(\"Product ID\"), F.col(\"Category\"), F.col(\"Sub-Category\")))).cast(\"long\"))\n",
    "               .withColumn(\"customer_sk\", F.abs(F.xxhash64(F.concat(F.col(\"Customer ID\"), F.col(\"Customer Name\"), F.col(\"Segment\")))).cast(\"long\"))\n",
    "               .drop( \"City\", \"State\", \"Country\", \"Region\", \"Product Name\", \"Product ID\", \"Category\", \"Sub-Category\", \"Customer ID\", \"Customer Name\", \"Segment\")\n",
    "                .dropDuplicates()\n",
    "              )\n",
    "\n",
    "# Writing the Delta files in silver schema\n",
    "# Writing order table\n",
    "\n",
    "(fact_orders.write.format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\",\"true\")\n",
    "  .option(\"delta.columnMapping.mode\",\"name\")\n",
    "  .option(\"delta.minReaderVersion\",\"2\")\n",
    "  .option(\"delta.minWriterVersion\",\"5\")\n",
    "  .save(orders_path) \n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3473ba0d-6f9d-49de-8ce0-0edc28f0a95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    " # Define the path of silver order\n",
    "orders_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_order/\"\n",
    "\n",
    "# Read the Delta file\n",
    "orders_df = spark.read.format(\"delta\").load(orders_path)\n",
    "\n",
    "# Set variable of minimum and maximum order dates\n",
    "Date_range = (orders_df\n",
    "              .select(F.min(\"Order_Date\").alias(\"start\"),\n",
    "                      F.max(\"Order_Date\").alias(\"end\"))\n",
    "              .first())\n",
    "\n",
    "# Extract start and end dates\n",
    "start = Date_range['start']\n",
    "end = Date_range['end']\n",
    "\n",
    "# Create sequence in the column with start date and end date of 1 day interval\n",
    "date_df = (spark.createDataFrame([(start, end)], [\"start\",\"end\"])\n",
    "           .select(F.explode(F.sequence(F.col(\"start\"), F.col(\"end\"), F.expr(\"interval 1 day\"))).alias(\"date\")))\n",
    "\n",
    "# Define necessary columns\n",
    "dow_iso = F.date_format(F.col(\"date\"), \"E\").cast(\"int\")\n",
    "fy_start_year = F.when(F.month(F.col(\"date\")) >= 4, F.year(F.col(\"date\"))).otherwise(F.year(F.col(\"date\")) - 1)\n",
    "fiscal_month_number = F.expr(\"((month(date) + 8) % 12) + 1\")\n",
    "fiscal_quarter = F.expr(\"((month(date) + 2) div 3) % 4 + 1\")\n",
    "\n",
    "# Create calendar columns on the basis of date column\n",
    "dim_date = (date_df\n",
    "  # keys\n",
    "  .withColumn(\"date_key\", F.date_format(\"date\",\"yyyyMMdd\").cast(\"int\"))\n",
    "\n",
    "  # calendar\n",
    "  .withColumn(\"year\", F.year(\"date\"))\n",
    "  .withColumn(\"quarter\", F.quarter(\"date\"))\n",
    "  .withColumn(\"month\", F.month(\"date\"))\n",
    "  .withColumn(\"day\",  F.dayofmonth(\"date\"))\n",
    "  .withColumn(\"weekofyear\",F.weekofyear(\"date\"))\n",
    "  .withColumn(\"day_name\",F.date_format(\"date\",\"EE\"))\n",
    "  .withColumn(\"month_name\", F.date_format(\"date\",\"MMM\"))\n",
    "  .withColumn(\"month_name_full\", F.date_format(\"date\",\"MMMM\"))\n",
    " \n",
    ")\n",
    "\n",
    "\n",
    "# Writing the Delta files in silver schema\n",
    "# Writing order table\n",
    "\n",
    "(dim_date.write.format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\",\"true\")\n",
    "  .option(\"delta.columnMapping.mode\",\"name\")\n",
    "  .option(\"delta.minReaderVersion\",\"2\")\n",
    "  .option(\"delta.minWriterVersion\",\"5\")\n",
    "  .save(date_path) \n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe9d7e1-1490-4fad-b4c6-a7fd28fed90f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754888148119}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# # Display the first 5 rows of orders_bz\n",
    "# display(orders_bz.limit(5))\n",
    "\n",
    "# # Display the first 5 rows of returns_bz\n",
    "# display(returns_bz.limit(5))\n",
    "\n",
    "# # Display the first 5 rows of dim_customer\n",
    "# display(dim_customer.limit(5))\n",
    "\n",
    "# # Display the first 5 rows of dim_product\n",
    "# display(dim_product.limit(5))\n",
    "\n",
    "# # Display the first 5 rows of dim_geography\n",
    "# display(dim_geography.limit(5))\n",
    "\n",
    "# # Display the first 5 rows of dim_returns\n",
    "# display(dim_returns.limit(5))\n",
    "\n",
    "# # Display the first 5 rows of fact_orders\n",
    "# display(fact_orders.limit(5))\n",
    "\n",
    "\n",
    "# display(dim_date.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8ad835-3928-4bce-9e7c-3aefc5c8857f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_customer/\").limit(5).display()\n",
    "# spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_geography/\").limit(5).display()\n",
    "# spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_product/\").limit(5).display()\n",
    "# spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_return/\").limit(5).display()\n",
    "# spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_order/\").limit(5).display()\n",
    "# spark.read.format(\"delta\").load(\"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_date/\").limit(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86f9fc3-6f6c-4555-b2e3-15be5c64bc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the paths\n",
    "customer_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_customer/\"\n",
    "geography_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_geography/\"\n",
    "product_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_product/\"\n",
    "return_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_return/\"\n",
    "order_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_order/\"\n",
    "date_path = \"dbfs:/Volumes/globalsuperstore/silver/silver_superstore/silver_date/\"\n",
    "\n",
    "# Create schema if not exists\n",
    "# spark.sql(\"CREATE SCHEMA IF NOT EXISTS globalsuperstore.silver_superstore_views\")\n",
    "\n",
    "# Create permanent views in the specified schema\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW globalsuperstore.silver_superstore_views.silver_customer AS SELECT * FROM delta.`{customer_path}`\")\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW globalsuperstore.silver_superstore_views.silver_geography AS SELECT DISTINCT city, state, country, region, geography_sk FROM delta.`{geography_path}`\")\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW globalsuperstore.silver_superstore_views.silver_product AS SELECT * FROM delta.`{product_path}`\")\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW globalsuperstore.silver_superstore_views.silver_return AS SELECT * FROM delta.`{return_path}`\")\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW globalsuperstore.silver_superstore_views.silver_order AS SELECT * FROM delta.`{order_path}`\")\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW globalsuperstore.silver_superstore_views.silver_date AS SELECT * FROM delta.`{date_path}`\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7805085052332190,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}