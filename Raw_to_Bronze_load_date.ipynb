{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbfc8bc-9497-474a-b8ec-9ee35b87bfe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1859\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1858\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[0;32m-> 1859\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\u001B[1;32m   1860\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n",
       "\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    270\u001B[0m     request: Any,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n",
       "\u001B[1;32m    278\u001B[0m         request,\n",
       "\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n",
       "\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n",
       "\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n",
       "\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n",
       "\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m    284\u001B[0m     )\n",
       "\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n",
       "\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n",
       "\u001B[1;32m    331\u001B[0m )\n",
       "\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n",
       "\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n",
       "\u001B[1;32m    316\u001B[0m         request,\n",
       "\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n",
       "\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n",
       "\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n",
       "\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n",
       "\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n",
       "\u001B[1;32m    322\u001B[0m     )\n",
       "\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m   1192\u001B[0m (\n",
       "\u001B[1;32m   1193\u001B[0m     state,\n",
       "\u001B[1;32m   1194\u001B[0m     call,\n",
       "\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n",
       "\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n",
       "\u001B[1;32m   1197\u001B[0m )\n",
       "\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n",
       "\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
       "\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAVAILABLE\n",
       "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-08-12T07:16:11.890278496+00:00\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6740002223286289>, line 75\u001B[0m\n",
       "\u001B[1;32m     55\u001B[0m returns_df \u001B[38;5;241m=\u001B[39m (spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     56\u001B[0m   \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     57\u001B[0m   \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPERMISSIVE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     61\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mingestion_ts\u001B[39m\u001B[38;5;124m\"\u001B[39m,F\u001B[38;5;241m.\u001B[39mfrom_utc_timestamp(F\u001B[38;5;241m.\u001B[39mcurrent_timestamp(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsia/Kolkata\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     62\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_metadata.file_path\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
       "\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Casting the Columns to actual data type\u001B[39;00m\n",
       "\u001B[1;32m     66\u001B[0m orders_df_clean \u001B[38;5;241m=\u001B[39m (orders_df\n",
       "\u001B[1;32m     67\u001B[0m                    \n",
       "\u001B[1;32m     68\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msales_str\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mregexp_replace(F\u001B[38;5;241m.\u001B[39mregexp_replace(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSales\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     69\u001B[0m \n",
       "\u001B[1;32m     70\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSales\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtry_cast(nullif(sales_str, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) as  decimal(18,4))\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     71\u001B[0m \n",
       "\u001B[1;32m     72\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msales_str\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     73\u001B[0m \n",
       "\u001B[1;32m     74\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mconcat(F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright(`Order Date`, 4)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(right(`Order Date`, 7), 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), ( F\u001B[38;5;241m.\u001B[39mwhen(\n",
       "\u001B[0;32m---> 75\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrder Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m10\u001B[39m,\n",
       "\u001B[1;32m     76\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Order Date`, 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     77\u001B[0m       )\u001B[38;5;241m.\u001B[39mwhen(\n",
       "\u001B[1;32m     78\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrder Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m9\u001B[39m,\n",
       "\u001B[1;32m     79\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Order Date`, 1)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     80\u001B[0m       )\u001B[38;5;241m.\u001B[39motherwise(F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;28;01mNone\u001B[39;00m)))))\n",
       "\u001B[1;32m     81\u001B[0m   \n",
       "\u001B[1;32m     82\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrder Date\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtry_cast(nullif(Date, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) as  Date)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     83\u001B[0m \n",
       "\u001B[1;32m     84\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     85\u001B[0m \n",
       "\u001B[1;32m     86\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mconcat(F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright(`Ship Date`, 4)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(right(`Ship Date`, 7), 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), ( F\u001B[38;5;241m.\u001B[39mwhen(\n",
       "\u001B[1;32m     87\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShip Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m10\u001B[39m,\n",
       "\u001B[1;32m     88\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Ship Date`, 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     89\u001B[0m       )\u001B[38;5;241m.\u001B[39mwhen(\n",
       "\u001B[1;32m     90\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShip Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m9\u001B[39m,\n",
       "\u001B[1;32m     91\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Ship Date`, 1)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     92\u001B[0m       )\u001B[38;5;241m.\u001B[39motherwise(F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;28;01mNone\u001B[39;00m)))))\n",
       "\u001B[1;32m     93\u001B[0m   \n",
       "\u001B[1;32m     94\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShip Date\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtry_cast(nullif(Date, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) as  Date)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     95\u001B[0m \n",
       "\u001B[1;32m     96\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     97\u001B[0m \n",
       "\u001B[1;32m     98\u001B[0m )\n",
       "\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m## Defining the source schema \u001B[39;00m\n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;66;03m# Defining the folder name for delta files \u001B[39;00m\n",
       "\u001B[1;32m    104\u001B[0m bronze_base \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/Volumes/globalsuperstore/bronze/bronze_superstore\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/utils.py:268\u001B[0m, in \u001B[0;36m_with_origin.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_remote\n",
       "\u001B[0;32m--> 268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(func, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__name__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m is_debugging_enabled():\n",
       "\u001B[1;32m    269\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_remote():\n",
       "\u001B[1;32m    270\u001B[0m         \u001B[38;5;28;01mglobal\u001B[39;00m current_origin\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/utils.py:62\u001B[0m, in \u001B[0;36mis_debugging_enabled\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     59\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mgetActiveSession()\n",
       "\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spark \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     61\u001B[0m     _enable_debugging_cache \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m---> 62\u001B[0m         spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\n",
       "\u001B[1;32m     63\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.databricks.pyspark.dataframeDebugging.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     64\u001B[0m         )\u001B[38;5;241m.\u001B[39mlower()\n",
       "\u001B[1;32m     65\u001B[0m         \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     66\u001B[0m     )\n",
       "\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     68\u001B[0m     _enable_debugging_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/conf.py:66\u001B[0m, in \u001B[0;36mRuntimeConf.get\u001B[0;34m(self, key, default)\u001B[0m\n",
       "\u001B[1;32m     62\u001B[0m     op_get_with_default \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGetWithDefault(\n",
       "\u001B[1;32m     63\u001B[0m         pairs\u001B[38;5;241m=\u001B[39m[proto\u001B[38;5;241m.\u001B[39mKeyValue(key\u001B[38;5;241m=\u001B[39mkey, value\u001B[38;5;241m=\u001B[39mcast(Optional[\u001B[38;5;28mstr\u001B[39m], default))]\n",
       "\u001B[1;32m     64\u001B[0m     )\n",
       "\u001B[1;32m     65\u001B[0m     operation \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get_with_default\u001B[38;5;241m=\u001B[39mop_get_with_default)\n",
       "\u001B[0;32m---> 66\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mconfig(operation)\n",
       "\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mpairs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m1\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1864\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1863\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1864\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2055\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\u001B[1;32m   2056\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n",
       "\u001B[1;32m   2057\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n",
       "\u001B[1;32m   2058\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2060\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   2061\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1857\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1855\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n",
       "\u001B[1;32m   1856\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 1857\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n",
       "\u001B[1;32m   1858\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[1;32m   1859\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n",
       "\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n",
       "\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n",
       "\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RetriesExceeded",
        "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "metadata": {
        "errorSummary": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "RETRIES_EXCEEDED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1859\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1858\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[0;32m-> 1859\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n\u001B[1;32m   1860\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    270\u001B[0m     request: Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n\u001B[1;32m    278\u001B[0m         request,\n\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    284\u001B[0m     )\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n\u001B[1;32m    331\u001B[0m )\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n\u001B[1;32m    316\u001B[0m         request,\n\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n\u001B[1;32m    322\u001B[0m     )\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m   1192\u001B[0m (\n\u001B[1;32m   1193\u001B[0m     state,\n\u001B[1;32m   1194\u001B[0m     call,\n\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001B[1;32m   1197\u001B[0m )\n\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
        "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-08-12T07:16:11.890278496+00:00\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-6740002223286289>, line 75\u001B[0m\n\u001B[1;32m     55\u001B[0m returns_df \u001B[38;5;241m=\u001B[39m (spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPERMISSIVE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     61\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mingestion_ts\u001B[39m\u001B[38;5;124m\"\u001B[39m,F\u001B[38;5;241m.\u001B[39mfrom_utc_timestamp(F\u001B[38;5;241m.\u001B[39mcurrent_timestamp(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsia/Kolkata\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     62\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_metadata.file_path\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Casting the Columns to actual data type\u001B[39;00m\n\u001B[1;32m     66\u001B[0m orders_df_clean \u001B[38;5;241m=\u001B[39m (orders_df\n\u001B[1;32m     67\u001B[0m                    \n\u001B[1;32m     68\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msales_str\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mregexp_replace(F\u001B[38;5;241m.\u001B[39mregexp_replace(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSales\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     69\u001B[0m \n\u001B[1;32m     70\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSales\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtry_cast(nullif(sales_str, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) as  decimal(18,4))\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     71\u001B[0m \n\u001B[1;32m     72\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msales_str\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     73\u001B[0m \n\u001B[1;32m     74\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mconcat(F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright(`Order Date`, 4)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(right(`Order Date`, 7), 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), ( F\u001B[38;5;241m.\u001B[39mwhen(\n\u001B[0;32m---> 75\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrder Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     76\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Order Date`, 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     77\u001B[0m       )\u001B[38;5;241m.\u001B[39mwhen(\n\u001B[1;32m     78\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrder Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m9\u001B[39m,\n\u001B[1;32m     79\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Order Date`, 1)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     80\u001B[0m       )\u001B[38;5;241m.\u001B[39motherwise(F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;28;01mNone\u001B[39;00m)))))\n\u001B[1;32m     81\u001B[0m   \n\u001B[1;32m     82\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrder Date\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtry_cast(nullif(Date, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) as  Date)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     83\u001B[0m \n\u001B[1;32m     84\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     85\u001B[0m \n\u001B[1;32m     86\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mconcat(F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright(`Ship Date`, 4)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(right(`Ship Date`, 7), 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m), ( F\u001B[38;5;241m.\u001B[39mwhen(\n\u001B[1;32m     87\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShip Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     88\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Ship Date`, 2)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     89\u001B[0m       )\u001B[38;5;241m.\u001B[39mwhen(\n\u001B[1;32m     90\u001B[0m           F\u001B[38;5;241m.\u001B[39mlength(F\u001B[38;5;241m.\u001B[39mtrim(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShip Date\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m9\u001B[39m,\n\u001B[1;32m     91\u001B[0m           F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft(`Ship Date`, 1)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     92\u001B[0m       )\u001B[38;5;241m.\u001B[39motherwise(F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;28;01mNone\u001B[39;00m)))))\n\u001B[1;32m     93\u001B[0m   \n\u001B[1;32m     94\u001B[0m   \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShip Date\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mexpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtry_cast(nullif(Date, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) as  Date)\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     95\u001B[0m \n\u001B[1;32m     96\u001B[0m   \u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     97\u001B[0m \n\u001B[1;32m     98\u001B[0m )\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m## Defining the source schema \u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;66;03m# Defining the folder name for delta files \u001B[39;00m\n\u001B[1;32m    104\u001B[0m bronze_base \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/Volumes/globalsuperstore/bronze/bronze_superstore\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/utils.py:268\u001B[0m, in \u001B[0;36m_with_origin.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_remote\n\u001B[0;32m--> 268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(func, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__name__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m is_debugging_enabled():\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_remote():\n\u001B[1;32m    270\u001B[0m         \u001B[38;5;28;01mglobal\u001B[39;00m current_origin\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/utils.py:62\u001B[0m, in \u001B[0;36mis_debugging_enabled\u001B[0;34m()\u001B[0m\n\u001B[1;32m     59\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mgetActiveSession()\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spark \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     61\u001B[0m     _enable_debugging_cache \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m---> 62\u001B[0m         spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m     63\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.databricks.pyspark.dataframeDebugging.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m         )\u001B[38;5;241m.\u001B[39mlower()\n\u001B[1;32m     65\u001B[0m         \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     66\u001B[0m     )\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     68\u001B[0m     _enable_debugging_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/conf.py:66\u001B[0m, in \u001B[0;36mRuntimeConf.get\u001B[0;34m(self, key, default)\u001B[0m\n\u001B[1;32m     62\u001B[0m     op_get_with_default \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGetWithDefault(\n\u001B[1;32m     63\u001B[0m         pairs\u001B[38;5;241m=\u001B[39m[proto\u001B[38;5;241m.\u001B[39mKeyValue(key\u001B[38;5;241m=\u001B[39mkey, value\u001B[38;5;241m=\u001B[39mcast(Optional[\u001B[38;5;28mstr\u001B[39m], default))]\n\u001B[1;32m     64\u001B[0m     )\n\u001B[1;32m     65\u001B[0m     operation \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get_with_default\u001B[38;5;241m=\u001B[39mop_get_with_default)\n\u001B[0;32m---> 66\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mconfig(operation)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mpairs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m1\u001B[39m]\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1864\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1863\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1864\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2055\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n\u001B[1;32m   2056\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n\u001B[1;32m   2057\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m   2058\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2060\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2061\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1857\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1855\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n\u001B[1;32m   1856\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1857\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n\u001B[1;32m   1858\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[1;32m   1859\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Defining the data type before or after laoding the data from csv \n",
    "#order csv file data type\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"Row ID\", IntegerType(), True),\n",
    "    StructField(\"Order ID\", StringType(), True),\n",
    "    StructField(\"Order Date\", StringType(), True),\n",
    "  StructField(\"Ship Date\", StringType(), True),\n",
    "    StructField(\"Ship Mode\", StringType(), True),\n",
    "    StructField(\"Customer ID\", StringType(), True),\n",
    "    StructField(\"Customer Name\", StringType(), True),\n",
    "    StructField(\"Segment\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Postal Code\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"Product ID\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Sub-Category\", StringType(), True),\n",
    "    StructField(\"Product Name\", StringType(), True),\n",
    "    StructField(\"Sales\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Discount\", DoubleType(), True),\n",
    "    StructField(\"Profit\", DoubleType(), True),\n",
    "    StructField(\"Shipping Cost\", DoubleType(), True),\n",
    "    StructField(\"Order Priority\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Return csv file data type\n",
    "\n",
    "returns_schema = StructType([\n",
    "    StructField(\"Returned\", StringType(), True),\n",
    "    StructField(\"Order ID\", StringType(), True),\n",
    "    StructField(\"Market\", StringType(), True),\n",
    "])\n",
    "\n",
    "## loading the CSV file into notebook \n",
    "# loading the order csv file \n",
    "\n",
    "orders_df = (spark.read.format(\"csv\")\n",
    "  .option(\"header\", True)\n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .schema(orders_schema)\n",
    "  .load(\"dbfs:/Volumes/globalsuperstore/raw_file/raw_superstore_order_volume/*.csv\")\n",
    "  .dropDuplicates()\n",
    "  .withColumn(\"ingestion_ts\",F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\"))\n",
    "  .withColumn(\"source_file\", F.col(\"_metadata.file_path\")))\n",
    "\n",
    "# Loading the return csv file\n",
    "\n",
    "returns_df = (spark.read.format(\"csv\")\n",
    "  .option(\"header\", True)\n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .schema(returns_schema)\n",
    "  .load(\"dbfs:/Volumes/globalsuperstore/raw_file/raw_superstore_return_volume/*.csv\")\n",
    "  .dropDuplicates()\n",
    "  .withColumn(\"ingestion_ts\",F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\"))\n",
    "  .withColumn(\"source_file\", F.col(\"_metadata.file_path\")))\n",
    "\n",
    "# Casting the Columns to actual data type\n",
    "\n",
    "orders_df_clean = (orders_df\n",
    "                   \n",
    "  .withColumn(\"sales_str\", F.regexp_replace(F.regexp_replace(F.col(\"Sales\"), r\"\\$\", \"\"), \",\", \"\"))\n",
    "\n",
    "  .withColumn(\"Sales\", F.expr(\"try_cast(nullif(sales_str, '') as  decimal(18,4))\"))\n",
    "\n",
    "  .drop(\"sales_str\")\n",
    "\n",
    "  .withColumn(\"Date\", F.concat(F.expr(\"right(`Order Date`, 4)\"), F.lit(\"-\"), F.expr(\"left(right(`Order Date`, 7), 2)\"), F.lit(\"-\"), ( F.when(\n",
    "          F.length(F.trim(F.col(\"Order Date\"))) == 10,\n",
    "          F.expr(\"left(`Order Date`, 2)\")\n",
    "      ).when(\n",
    "          F.length(F.trim(F.col(\"Order Date\"))) == 9,\n",
    "          F.expr(\"left(`Order Date`, 1)\")\n",
    "      ).otherwise(F.lit(None)))))\n",
    "  \n",
    "  .withColumn(\"Order Date\", F.expr(\"try_cast(nullif(Date, '') as  Date)\"))\n",
    "\n",
    "  .drop(\"Date\")\n",
    "\n",
    "  .withColumn(\"Date\", F.concat(F.expr(\"right(`Ship Date`, 4)\"), F.lit(\"-\"), F.expr(\"left(right(`Ship Date`, 7), 2)\"), F.lit(\"-\"), ( F.when(\n",
    "          F.length(F.trim(F.col(\"Ship Date\"))) == 10,\n",
    "          F.expr(\"left(`Ship Date`, 2)\")\n",
    "      ).when(\n",
    "          F.length(F.trim(F.col(\"Ship Date\"))) == 9,\n",
    "          F.expr(\"left(`Ship Date`, 1)\")\n",
    "      ).otherwise(F.lit(None)))))\n",
    "  \n",
    "  .withColumn(\"Ship Date\", F.expr(\"try_cast(nullif(Date, '') as  Date)\"))\n",
    "\n",
    "  .drop(\"Date\")\n",
    "\n",
    ")\n",
    "\n",
    "  \n",
    "## Defining the source schema \n",
    "# Defining the folder name for delta files \n",
    "\n",
    "bronze_base = \"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore\"\n",
    "orders_path  = f\"{bronze_base}/orders_bronze_cm\"\n",
    "returns_path = f\"{bronze_base}/returns_bronze_cm\"\n",
    "\n",
    "## Writing the Delta files in Bronze schema\n",
    "# Writing order table\n",
    "\n",
    "(orders_df_clean.write.format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\",\"true\")\n",
    "  .option(\"delta.columnMapping.mode\",\"name\")\n",
    "  .option(\"delta.minReaderVersion\",\"2\")\n",
    "  .option(\"delta.minWriterVersion\",\"5\")\n",
    "  .save(orders_path))\n",
    "\n",
    "# Writing return table\n",
    "\n",
    "(returns_df.write.format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\",\"true\")\n",
    "  .option(\"delta.columnMapping.mode\",\"name\")\n",
    "  .option(\"delta.minReaderVersion\",\"2\")\n",
    "  .option(\"delta.minWriterVersion\",\"5\")\n",
    "  .save(returns_path))\n",
    "\n",
    "\n",
    "spark.sql(f\"OPTIMIZE delta.`{orders_path.rstrip('/')}` ZORDER BY (`Order Date`, `Order ID`, `Customer ID`)\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{returns_path.rstrip('/')}`\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4848c4-39d7-439b-89c4-34f1cba33eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39054ae9-9d3c-490b-92a6-941150448001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Define the paths\n",
    "# orders_path = \"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore/orders_bronze_cm/\"\n",
    "# returns_path = \"dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore/returns_bronze_cm/\"\n",
    "\n",
    "# # Read the Delta files\n",
    "# orders_df = spark.read.format(\"delta\").load(orders_path)\n",
    "# returns_df = spark.read.format(\"delta\").load(returns_path)\n",
    "\n",
    "# # Display the DataFrames\n",
    "# orders_df_clean.limit(5).display()\n",
    "# orders_df.limit(5).display()\n",
    "# returns_df.limit(5).display()\n",
    "# display(orders_df.limit(5))\n",
    "# display(returns_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9dcfe7-b102-439b-9732-14d53eec7def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE OR REPLACE VIEW globalsuperstore.bronze.master_superstore AS SELECT * FROM delta.`dbfs:/Volumes/globalsuperstore/bronze/bronze_superstore/orders_bronze_cm/`\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Raw_to_Bronze_load_date",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}